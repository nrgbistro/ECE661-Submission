{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from models import Generator\n",
    "from torchmetrics.wrappers import FeatureShare\n",
    "from torchmetrics.image import FrechetInceptionDistance, KernelInceptionDistance, InceptionScore\n",
    "from torchmetrics.functional.image import perceptual_path_length\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 200\n",
    "\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "cifar10_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(cifar10_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_inception_metrics(generator, num_samples=100, fid_features=2048, label=None):\n",
    "    fs = FeatureShare([FrechetInceptionDistance(feature=fid_features), KernelInceptionDistance(subset_size=100)]).to(device)\n",
    "    fs.reset()\n",
    "    \n",
    "    inception = InceptionScore().to(device)\n",
    "    inception.reset()\n",
    "    \n",
    "    original_label = label\n",
    "    \n",
    "    i = 0\n",
    "    while i < num_samples:\n",
    "        real_images_batch = []\n",
    "        fake_images_batch = []\n",
    "        for _, (real_images, real_labels) in enumerate(train_loader):\n",
    "            real_images = real_images.to(device)\n",
    "            if not original_label:\n",
    "                label = torch.randint(low=0, high=10, size=(real_images.size(0),))\n",
    "            else:\n",
    "                label = torch.tensor([original_label])\n",
    "                mask = real_labels == label\n",
    "                real_images = real_images[mask]\n",
    "                label = torch.tensor([original_label] * real_images.size(0))\n",
    "            label = label.to(device)\n",
    "            one_hot_labels = F.one_hot(label, 10)\n",
    "            input_vector = torch.cat((torch.randn(real_images.size(0), 100).to(device), one_hot_labels.float()), dim=1)\n",
    "            fake_images = generator(input_vector)\n",
    "\n",
    "            # Resize real and fake images to 32x32\n",
    "            real_images = F.interpolate(real_images, size=32, mode='bilinear', align_corners=False)\n",
    "            fake_images = F.interpolate(fake_images, size=32, mode='bilinear', align_corners=False)\n",
    "\n",
    "            # Denormalize the images (assuming they were normalized to [-1, 1])\n",
    "            real_images = ((real_images + 1) / 2 * 255).to(torch.uint8)\n",
    "            fake_images = ((fake_images + 1) / 2 * 255).to(torch.uint8)\n",
    "\n",
    "            real_images_batch.append(real_images)\n",
    "            fake_images_batch.append(fake_images)\n",
    "\n",
    "            if len(real_images_batch) >= BATCH_SIZE:\n",
    "                break\n",
    "\n",
    "        # Concatenate batches if there are enough images, else continue to next batch\n",
    "        if len(real_images_batch) >= BATCH_SIZE:\n",
    "            real_images_batch = torch.cat(real_images_batch, dim=0)[:BATCH_SIZE]\n",
    "            fake_images_batch = torch.cat(fake_images_batch, dim=0)[:BATCH_SIZE]\n",
    "            fs.update(real_images_batch, True)\n",
    "            fs.update(fake_images_batch, False)\n",
    "            inception.update(fake_images_batch)\n",
    "            i += len(real_images_batch)\n",
    "\n",
    "        print(f\"Processed {i}/{num_samples} samples\")\n",
    "\n",
    "    score = fs.compute()\n",
    "    inception_score = inception.compute()\n",
    "    score[\"InceptionScore\"] = inception_score\n",
    "    print(score)\n",
    "    return score[\"FrechetInceptionDistance\"], score[\"KernelInceptionDistance\"], score[\"InceptionScore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(100).to(device)\n",
    "generator.load_state_dict(torch.load(\"final_models/final_generator.pth\"))\n",
    "\n",
    "avg_fid = 0\n",
    "avg_kid_mean = 0\n",
    "avg_kid_std = 0\n",
    "avg_is_mean = 0\n",
    "avg_is_std = 0\n",
    "for _ in range(3):\n",
    "    fid, kid, inception = compute_inception_metrics(generator, num_samples=200, label=7)\n",
    "    avg_fid += fid\n",
    "    avg_kid_mean += kid[0]\n",
    "    avg_kid_std += kid[1]\n",
    "    avg_is_mean += inception[0]\n",
    "    avg_is_std += inception[1]\n",
    "print(f\"Average FID: {avg_fid / 3}\")\n",
    "print(f\"Average KID mean: {avg_kid_mean / 3}, Average KID std: {avg_kid_std / 3}\")\n",
    "print(f\"Average IS mean: {avg_is_mean / 3}, Average IS std: {avg_is_std / 3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorWrapper(Generator):\n",
    "    def __init__(self, z_size, num_classes=10):\n",
    "        self.latent_size = z_size\n",
    "        super().__init__(z_size, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return 255 * (super().forward(z) * 0.5 + 0.5)\n",
    "    \n",
    "    def sample(self, num_samples):\n",
    "        label = torch.randint(0, 10, (num_samples,))\n",
    "        one_hot_labels = F.one_hot(label, 10)\n",
    "        noise = torch.randn(num_samples, self.latent_size, device=self.fc1.weight.device)\n",
    "        input_vector = torch.cat((noise, one_hot_labels.float()), dim=1)\n",
    "        return input_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(39.2204),\n",
       " tensor(27.6649),\n",
       " tensor([ 32.2516,  21.8606,  35.5805,  56.8346, 103.4226,   9.8458,  45.6617,\n",
       "          97.4959,  30.9462,  21.3177, 120.9710,  60.5314,  23.0506,  17.4871,\n",
       "          37.4160,  86.5625,  25.3680,  12.3684,  14.4844,  33.7865,  20.4293,\n",
       "          18.6896,  18.6774,  36.0100,  23.8001,  18.8495,  25.5616,  42.2301,\n",
       "          14.0406,  50.3401,  29.9364,  66.0843,  28.7563,  62.2913,  27.8636,\n",
       "          33.1642,  12.3069,  33.7130,  33.2749,  38.8485,  29.1089,  42.7706,\n",
       "          78.3030,  41.3270,  37.8709,  15.1425,  45.8934,  26.6008,  42.9832,\n",
       "          21.2979,  22.9904,  56.7789,  37.0110,  44.1315,  17.6006,  11.0275,\n",
       "          13.7830,  29.3422,  11.6531,   9.0502,  36.4368,  13.6588, 129.3887,\n",
       "          60.8706,  13.0150,  54.9913,  53.8828,  50.6271,  11.9265,  64.1133,\n",
       "          23.0048,  48.7219,  55.5936,  46.1058,  43.3842,  34.6218,  31.3851,\n",
       "          45.9221,  10.3538,  24.9722,  49.8425,  21.5106,  52.3321,   9.1489,\n",
       "          87.8503,  94.5883,  46.8018, 170.4241,  24.1119,  34.4055,  44.4447,\n",
       "          46.7816,  61.6821,  40.8900,  14.3795,  30.8640,  24.7113,   2.9659,\n",
       "          15.3337]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = GeneratorWrapper(100).to(device)\n",
    "generator.load_state_dict(torch.load(\"final_models/final_generator.pth\"))\n",
    "generator = generator.to(device)\n",
    "generator.eval()\n",
    "\n",
    "perceptual_path_length(generator, num_samples=100, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
