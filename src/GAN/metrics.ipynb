{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from models import Generator\n",
    "from torchmetrics.wrappers import FeatureShare\n",
    "from torchmetrics.image import FrechetInceptionDistance, KernelInceptionDistance, InceptionScore, PerceptualPathLength\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional.image import perceptual_path_length\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 200\n",
    "\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "cifar10_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(cifar10_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_inception_metrics(generator, num_samples=100, fid_features=2048):\n",
    "    fs = FeatureShare([FrechetInceptionDistance(feature=fid_features), KernelInceptionDistance(subset_size=100)]).to(device)\n",
    "    fs.reset()\n",
    "    \n",
    "    inception = InceptionScore().to(device)\n",
    "    inception.reset()\n",
    "    \n",
    "    i = 0\n",
    "    while i < num_samples:\n",
    "        real_images_batch = []\n",
    "        fake_images_batch = []\n",
    "        for _, (real_images, _) in enumerate(train_loader):\n",
    "            real_images = real_images.to(device)\n",
    "            fake_images = generator(torch.randn(real_images.size(0), 100).to(device))\n",
    "\n",
    "            # Resize real and fake images to 32x32\n",
    "            real_images = F.interpolate(real_images, size=32, mode='bilinear', align_corners=False)\n",
    "            fake_images = F.interpolate(fake_images, size=32, mode='bilinear', align_corners=False)\n",
    "\n",
    "            # Denormalize the images (assuming they were normalized to [-1, 1])\n",
    "            real_images = ((real_images + 1) / 2 * 255).to(torch.uint8)\n",
    "            fake_images = ((fake_images + 1) / 2 * 255).to(torch.uint8)\n",
    "\n",
    "            real_images_batch.append(real_images)\n",
    "            fake_images_batch.append(fake_images)\n",
    "\n",
    "            if len(real_images_batch) >= BATCH_SIZE:\n",
    "                break\n",
    "\n",
    "        # Concatenate batches if there are enough images, else continue to next batch\n",
    "        if len(real_images_batch) >= BATCH_SIZE:\n",
    "            real_images_batch = torch.cat(real_images_batch, dim=0)[:BATCH_SIZE]\n",
    "            fake_images_batch = torch.cat(fake_images_batch, dim=0)[:BATCH_SIZE]\n",
    "            fs.update(real_images_batch, True)\n",
    "            fs.update(fake_images_batch, False)\n",
    "            inception.update(fake_images_batch)\n",
    "            i += len(real_images_batch)\n",
    "\n",
    "        print(f\"Processed {i}/{num_samples} samples\")\n",
    "\n",
    "    score = fs.compute()\n",
    "    inception_score = inception.compute()\n",
    "    score[\"InceptionScore\"] = inception_score\n",
    "    print(score)\n",
    "    return score[\"FrechetInceptionDistance\"], score[\"KernelInceptionDistance\"], score[\"InceptionScore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nrg23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Metric `Kernel Inception Distance` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "c:\\Users\\nrg23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200/10000 samples\n",
      "Processed 400/10000 samples\n",
      "Processed 600/10000 samples\n",
      "Processed 800/10000 samples\n",
      "Processed 1000/10000 samples\n",
      "Processed 1200/10000 samples\n",
      "Processed 1400/10000 samples\n",
      "Processed 1600/10000 samples\n",
      "Processed 1800/10000 samples\n",
      "Processed 2000/10000 samples\n",
      "Processed 2200/10000 samples\n",
      "Processed 2400/10000 samples\n",
      "Processed 2600/10000 samples\n",
      "Processed 2800/10000 samples\n",
      "Processed 3000/10000 samples\n",
      "Processed 3200/10000 samples\n",
      "Processed 3400/10000 samples\n",
      "Processed 3600/10000 samples\n",
      "Processed 3800/10000 samples\n",
      "Processed 4000/10000 samples\n",
      "Processed 4200/10000 samples\n",
      "Processed 4400/10000 samples\n",
      "Processed 4600/10000 samples\n",
      "Processed 4800/10000 samples\n",
      "Processed 5000/10000 samples\n",
      "Processed 5200/10000 samples\n",
      "Processed 5400/10000 samples\n",
      "Processed 5600/10000 samples\n",
      "Processed 5800/10000 samples\n",
      "Processed 6000/10000 samples\n",
      "Processed 6200/10000 samples\n",
      "Processed 6400/10000 samples\n",
      "Processed 6600/10000 samples\n",
      "Processed 6800/10000 samples\n",
      "Processed 7000/10000 samples\n",
      "Processed 7200/10000 samples\n",
      "Processed 7400/10000 samples\n",
      "Processed 7600/10000 samples\n",
      "Processed 7800/10000 samples\n",
      "Processed 8000/10000 samples\n",
      "Processed 8200/10000 samples\n",
      "Processed 8400/10000 samples\n",
      "Processed 8600/10000 samples\n",
      "Processed 8800/10000 samples\n",
      "Processed 9000/10000 samples\n",
      "Processed 9200/10000 samples\n",
      "Processed 9400/10000 samples\n",
      "Processed 9600/10000 samples\n",
      "Processed 9800/10000 samples\n",
      "Processed 10000/10000 samples\n",
      "{'FrechetInceptionDistance': tensor(242.3947, device='cuda:0'), 'KernelInceptionDistance': (tensor(0.2554, device='cuda:0'), tensor(0.0167, device='cuda:0')), 'InceptionScore': (tensor(2.0836, device='cuda:0'), tensor(0.0535, device='cuda:0'))}\n",
      "Processed 200/10000 samples\n",
      "Processed 400/10000 samples\n",
      "Processed 600/10000 samples\n",
      "Processed 800/10000 samples\n",
      "Processed 1000/10000 samples\n",
      "Processed 1200/10000 samples\n",
      "Processed 1400/10000 samples\n",
      "Processed 1600/10000 samples\n",
      "Processed 1800/10000 samples\n",
      "Processed 2000/10000 samples\n",
      "Processed 2200/10000 samples\n",
      "Processed 2400/10000 samples\n",
      "Processed 2600/10000 samples\n",
      "Processed 2800/10000 samples\n",
      "Processed 3000/10000 samples\n",
      "Processed 3200/10000 samples\n",
      "Processed 3400/10000 samples\n",
      "Processed 3600/10000 samples\n",
      "Processed 3800/10000 samples\n",
      "Processed 4000/10000 samples\n",
      "Processed 4200/10000 samples\n",
      "Processed 4400/10000 samples\n",
      "Processed 4600/10000 samples\n",
      "Processed 4800/10000 samples\n",
      "Processed 5000/10000 samples\n",
      "Processed 5200/10000 samples\n",
      "Processed 5400/10000 samples\n",
      "Processed 5600/10000 samples\n",
      "Processed 5800/10000 samples\n",
      "Processed 6000/10000 samples\n",
      "Processed 6200/10000 samples\n",
      "Processed 6400/10000 samples\n",
      "Processed 6600/10000 samples\n",
      "Processed 6800/10000 samples\n",
      "Processed 7000/10000 samples\n",
      "Processed 7200/10000 samples\n",
      "Processed 7400/10000 samples\n",
      "Processed 7600/10000 samples\n",
      "Processed 7800/10000 samples\n",
      "Processed 8000/10000 samples\n",
      "Processed 8200/10000 samples\n",
      "Processed 8400/10000 samples\n",
      "Processed 8600/10000 samples\n",
      "Processed 8800/10000 samples\n",
      "Processed 9000/10000 samples\n",
      "Processed 9200/10000 samples\n",
      "Processed 9400/10000 samples\n",
      "Processed 9600/10000 samples\n",
      "Processed 9800/10000 samples\n",
      "Processed 10000/10000 samples\n",
      "{'FrechetInceptionDistance': tensor(242.3179, device='cuda:0'), 'KernelInceptionDistance': (tensor(0.2544, device='cuda:0'), tensor(0.0160, device='cuda:0')), 'InceptionScore': (tensor(2.1125, device='cuda:0'), tensor(0.0272, device='cuda:0'))}\n",
      "Processed 200/10000 samples\n",
      "Processed 400/10000 samples\n",
      "Processed 600/10000 samples\n",
      "Processed 800/10000 samples\n",
      "Processed 1000/10000 samples\n",
      "Processed 1200/10000 samples\n",
      "Processed 1400/10000 samples\n",
      "Processed 1600/10000 samples\n",
      "Processed 1800/10000 samples\n",
      "Processed 2000/10000 samples\n",
      "Processed 2200/10000 samples\n",
      "Processed 2400/10000 samples\n",
      "Processed 2600/10000 samples\n",
      "Processed 2800/10000 samples\n",
      "Processed 3000/10000 samples\n",
      "Processed 3200/10000 samples\n",
      "Processed 3400/10000 samples\n",
      "Processed 3600/10000 samples\n",
      "Processed 3800/10000 samples\n",
      "Processed 4000/10000 samples\n",
      "Processed 4200/10000 samples\n",
      "Processed 4400/10000 samples\n",
      "Processed 4600/10000 samples\n",
      "Processed 4800/10000 samples\n",
      "Processed 5000/10000 samples\n",
      "Processed 5200/10000 samples\n",
      "Processed 5400/10000 samples\n",
      "Processed 5600/10000 samples\n",
      "Processed 5800/10000 samples\n",
      "Processed 6000/10000 samples\n",
      "Processed 6200/10000 samples\n",
      "Processed 6400/10000 samples\n",
      "Processed 6600/10000 samples\n",
      "Processed 6800/10000 samples\n",
      "Processed 7000/10000 samples\n",
      "Processed 7200/10000 samples\n",
      "Processed 7400/10000 samples\n",
      "Processed 7600/10000 samples\n",
      "Processed 7800/10000 samples\n",
      "Processed 8000/10000 samples\n",
      "Processed 8200/10000 samples\n",
      "Processed 8400/10000 samples\n",
      "Processed 8600/10000 samples\n",
      "Processed 8800/10000 samples\n",
      "Processed 9000/10000 samples\n",
      "Processed 9200/10000 samples\n",
      "Processed 9400/10000 samples\n",
      "Processed 9600/10000 samples\n",
      "Processed 9800/10000 samples\n",
      "Processed 10000/10000 samples\n",
      "{'FrechetInceptionDistance': tensor(242.8734, device='cuda:0'), 'KernelInceptionDistance': (tensor(0.2541, device='cuda:0'), tensor(0.0171, device='cuda:0')), 'InceptionScore': (tensor(2.0948, device='cuda:0'), tensor(0.0345, device='cuda:0'))}\n",
      "Average FID: 242.52867126464844\n",
      "Average KID mean: 0.254619836807251, Average KID std: 0.016606193035840988\n",
      "Average IS mean: 2.0969772338867188, Average IS std: 0.03840051218867302\n"
     ]
    }
   ],
   "source": [
    "generator = Generator(100).to(device)\n",
    "generator.load_state_dict(torch.load(\"final_models/final_generator.pth\"))\n",
    "\n",
    "avg_fid = 0\n",
    "avg_kid_mean = 0\n",
    "avg_kid_std = 0\n",
    "avg_is_mean = 0\n",
    "avg_is_std = 0\n",
    "for _ in range(3):\n",
    "    fid, kid, inception = compute_inception_metrics(generator, num_samples=10000)\n",
    "    avg_fid += fid\n",
    "    avg_kid_mean += kid[0]\n",
    "    avg_kid_std += kid[1]\n",
    "    avg_is_mean += inception[0]\n",
    "    avg_is_std += inception[1]\n",
    "print(f\"Average FID: {avg_fid / 3}\")\n",
    "print(f\"Average KID mean: {avg_kid_mean / 3}, Average KID std: {avg_kid_std / 3}\")\n",
    "print(f\"Average IS mean: {avg_is_mean / 3}, Average IS std: {avg_is_std / 3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [100, 1024, 4, 4], expected input[128, 3, 32, 32] to have 100 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m wrapped_generator \u001b[38;5;241m=\u001b[39m WrappedGenerator(\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m wrapped_generator\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_models/final_generator.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 17\u001b[0m \u001b[43mcompute_ppl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m, in \u001b[0;36mcompute_ppl\u001b[1;34m(model, num_samples)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_ppl\u001b[39m(model, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m      2\u001b[0m     ppl \u001b[38;5;241m=\u001b[39m PerceptualPathLength(num_samples\u001b[38;5;241m=\u001b[39mnum_samples)\n\u001b[1;32m----> 3\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mppl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[1;32mc:\\Users\\nrg23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nrg23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nrg23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchmetrics\\metric.py:302\u001b[0m, in \u001b[0;36mMetric.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TorchMetricsUserError(\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Metric shouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be synced when performing ``forward``. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHINT: Did you forget to call ``unsync`` ?.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m     )\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_state_update \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_state_update \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_sync_on_step:\n\u001b[1;32m--> 302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_full_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_reduce_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nrg23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchmetrics\\metric.py:334\u001b[0m, in \u001b[0;36mMetric._forward_full_state_update\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 334\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;66;03m# restore context\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr, val \u001b[38;5;129;01min\u001b[39;00m cache\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\nrg23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchmetrics\\metric.py:616\u001b[0m, in \u001b[0;36mMetric._wrap_compute.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;66;03m# compute relies on the sync context manager to gather the states across processes and apply reduction\u001b[39;00m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;66;03m# if synchronization happened, the current rank accumulated states will be restored to keep\u001b[39;00m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# accumulation going if ``should_unsync=True``,\u001b[39;00m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msync_context(\n\u001b[0;32m    612\u001b[0m     dist_sync_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_sync_fn,\n\u001b[0;32m    613\u001b[0m     should_sync\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_sync,\n\u001b[0;32m    614\u001b[0m     should_unsync\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_unsync,\n\u001b[0;32m    615\u001b[0m ):\n\u001b[1;32m--> 616\u001b[0m     value \u001b[38;5;241m=\u001b[39m _squeeze_if_scalar(\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_with_cache:\n\u001b[0;32m    619\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[1;32mc:\\Users\\nrg23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchmetrics\\image\\perceptual_path_length.py:174\u001b[0m, in \u001b[0;36mPerceptualPathLength.compute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor, Tensor]:\n\u001b[0;32m    173\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the perceptual path length.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mperceptual_path_length\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconditional\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconditional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlower_discard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower_discard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupper_discard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupper_discard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43msim_net\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nrg23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchmetrics\\functional\\image\\perceptual_path_length.py:262\u001b[0m, in \u001b[0;36mperceptual_path_length\u001b[1;34m(generator, num_samples, conditional, batch_size, interpolation_method, epsilon, resize, lower_discard, upper_discard, sim_net, device)\u001b[0m\n\u001b[0;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m generator(\n\u001b[0;32m    259\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcat((batch_latent1, batch_latent2), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), torch\u001b[38;5;241m.\u001b[39mcat((batch_labels, batch_labels), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    260\u001b[0m     )\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_latent1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_latent2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m out1, out2 \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# rescale to lpips expected domain: [0, 255] -> [0, 1] -> [-1, 1]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nrg23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nrg23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nrg23\\Downloads\\ECE661-GAN\\src\\WGAN\\models.py:31\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nrg23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nrg23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nrg23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nrg23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nrg23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nrg23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:952\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    947\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    948\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    950\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [100, 1024, 4, 4], expected input[128, 3, 32, 32] to have 100 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "def compute_ppl(model, num_samples=100):\n",
    "    ppl = PerceptualPathLength(num_samples=num_samples)\n",
    "    result = ppl(model)\n",
    "    print(result)\n",
    "\n",
    "class WrappedGenerator(Generator):\n",
    "    def __init__(self, latent_size):\n",
    "        super(WrappedGenerator, self).__init__(latent_size)\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        noise = torch.randn(num_samples, self.latent_size, 1, 1, device=self.main[0].weight.device)\n",
    "        with torch.no_grad():\n",
    "            samples = self.forward(noise)\n",
    "        return samples\n",
    "\n",
    "\n",
    "\n",
    "wrapped_generator = WrappedGenerator(100).to(device)\n",
    "wrapped_generator.load_state_dict(torch.load(\"final_models/final_generator.pth\"))\n",
    "\n",
    "compute_ppl(wrapped_generator, num_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorWrapper(Generator):\n",
    "    def __init__(self, z_size):\n",
    "        super().__init__(z_size)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return 255 * (super().forward(z) * 0.5 + 0.5)\n",
    "    \n",
    "    def sample(self, num_samples):\n",
    "        noise = torch.randn(\n",
    "            num_samples, self.latent_size, device=self.model[0].weight.device\n",
    "        )\n",
    "        return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 18.0496883392334, Std: 19.856122970581055\n"
     ]
    }
   ],
   "source": [
    "generator = GeneratorWrapper(100).to(device)\n",
    "generator.load_state_dict(torch.load(\"final_models/final_generator.pth\"))\n",
    "generator = generator.to(device)\n",
    "generator.eval()\n",
    "\n",
    "mean, std, _ = perceptual_path_length(generator)\n",
    "print(f\"Mean: {mean}, Std: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
